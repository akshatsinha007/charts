# Default values for devtron-monitoring.
fluent-bit:
  enabled: false
  serviceMonitor:
    enabled: true
  resources: {}
  logLevel: debug
  podAnnotations: 
    fluentbit.io/exclude: "true"
  env:  []
  # - name: AWS_ACCESS_KEY_ID
  #   value: ""
  # - name: AWS_SECRET_ACCESS_KEY
  #   value: ""  
  config:
    inputs: |
      [INPUT]
          Name tail
          Path /var/log/containers/*.log
          Exclude_Path /var/log/containers/*_utils_*.log , *.flb
          multiline.parser docker, cri
          Tag_Regex (?<pod_name>[a-z0-9](?:[-a-z0-9]*[a-z0-9])?(?:\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace_name>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\.log$
          Tag kube.<namespace_name>.<pod_name>.<container_name>
          Mem_Buf_Limit 50MB
          Skip_Long_Lines On
          DB /var/log/flb_kube.db
          Refresh_Interval  60
      [INPUT]
          Name tail
          Path /var/log/containers/*orchestrator*.log
          multiline.parser docker, cri
          Tag audit.*
          Mem_Buf_Limit 150MB
          Skip_Long_Lines On
          Parser docker
          DB /var/log/flb_kube.db
          Refresh_Interval  30
      [INPUT]
          Name systemd
          Tag host.*
          Systemd_Filter _SYSTEMD_UNIT=kubelet.service
          Read_From_Tail On
      [INPUT]
          Name tail
          Path /var/log/containers/*_devtroncd_*.log , /var/log/containers/*_devtron-ci_*.log 
          Exclude_Path /var/log/containers/*_utils_*.log , *.flb
          multiline.parser docker,cri 
          Tag_Regex (?<pod_name>[a-z0-9](?:[-a-z0-9]*[a-z0-9])?(?:\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace_name>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\.log$
          Tag error.<namespace_name>.<pod_name>.<container_name>
          Mem_buf_Limit 50MB
          Parser docker
          DB /var/log/flb_kube.db
          Refresh_Interval 30
      [INPUT]
          Name tail
          Path /var/log/containers/*_devtroncd_*.log , /var/log/containers/*_devtron-ci_*.log 
          Exclude_Path /var/log/containers/*_utils_*.log , *.flb
          multiline.parser docker,cri 
          Tag_Regex (?<pod_name>[a-z0-9](?:[-a-z0-9]*[a-z0-9])?(?:\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace_name>[^_]+)_(?<container_name>.+)-(?<docker_id>[a-z0-9]{64})\.log$
          Tag nats.<namespace_name>.<pod_name>.<container_name>
          Mem_buf_Limit 50MB
          Parser docker
          DB /var/log/flb_kube.db
          Refresh_Interval 30

    ## https://docs.fluentbit.io/manual/pipeline/filters
    filters: |
      [FILTER]
          Name kubernetes
          Match kube.*
          Kube_Tag_Prefix kube.
          Merge_Log On
          Merge_Log_Key log_processed
          Keep_Log Off
          K8S-Logging.Parser On
          K8S-Logging.Exclude On
      [FILTER]
          Name grep
          Match audit.*
          Regex log .*AUDIT_LOG:.*urlPath:\s.[^(health|metrics)].*   
      [FILTER]
          Name grep
          Match nats.*
          Regex log .*NATS_LOG.*
      [FILTER]
          Name grep
          Match error.*
          Regex log .*(DEVTRON_PANIC_RECOVER|nats: found panic error).*      

    ## https://docs.fluentbit.io/manual/pipeline/outputs
    outputs: |
      [OUTPUT]
          Name s3
          Match kube.*
          region <bucket_region>
          endpoint https://<add if any custom endpoint>
          bucket <bucket_name>
          s3_key_format /$TAG[1]/$TAG[3]/%Y-%m-%d/%H_%M_%S_$TAG[2].log
          # s3_key_format /$TAG[1]/$TAG[3]/%Y-%m-%d/%H_%M_%S.log
          s3_key_format_tag_delimiters .
          static_file_path On
          use_put_object Off
          upload_timeout 10m
          storage_class GLACIER_IR 
      [OUTPUT]
          Name s3
          Match audit.*
          region <bucket_region>
          endpoint https://<add if any custom endpoint>
          bucket <bucket_name>
          s3_key_format /audit_logs/orchestrator/%Y-%m-%d/%H_%M_%S.log
          s3_key_format_tag_delimiters .
          static_file_path On
          use_put_object Off
          upload_timeout 10m
          storage_class GLACIER_IR    
      [OUTPUT]
          Name s3
          Match nats.*
          region  <bucket_region>
          endpoint https://<add if any custom endpoint>
          bucket  <bucket_name>
          s3_key_format /nats_pub_sub/$TAG[1]/$TAG[3]/%Y-%m-%d/%H_%M_%S.log
          s3_key_format_tag_delimiters .
          static_file_path On
          use_put_object Off
          upload_timeout 10m
          storage_class GLACIER_IR   
      [OUTPUT]
          Name s3
          Match error.*
          region  <bucket_region>
          bucket  <bucket_name>
          s3_key_format /panic/$TAG[1]/$TAG[3]/%Y-%m-%d/%H_%M_%S.log
          s3_key_format_tag_delimiters .
          static_file_path On
          use_put_object Off
          upload_timeout 10m
          storage_class GLACIER_IR 

vector:
  enabled: false
  resources: {}
  role: "Agent"
  service:
    ports: 
      - name: http
        port: 9090
  customConfig: 
    data_dir: /var/lib/vector
    sources:
      kube_log:
        type: "kubernetes_logs"
        timezone: "Asia/Kolkata"
        auto_partial_merge: true
        extra_namespace_label_selector: "kubernetes.io/metadata.name in (devtroncd)"
    transforms:
      filter_audit_logs:
        type: filter
        inputs:
          - kube_log
        condition: 'string!(.message) != null && contains(string!(.message), "AUDIT_LOG")'

      my_remap_id_audit:
        type: remap
        inputs:
          - filter_audit_logs
        source: |-
          .test = del(.)
          .container_name = del(.test.kubernetes.container_name)
          .message = del(.test.message)
          .timestamp = del(.test.timestamp)
          del(.test)

      my_remap_id:
        type: remap
        inputs:
        - kube_log
        source: |-
          .test = del(.)
          .container_name = del(.test.kubernetes.container_name)
          .message = del(.test.message)
          .timestamp = del(.test.timestamp)
          del(.test)


    sinks:
      my_sink_id:
        type: aws_s3
        inputs:
          - my_remap_id_audit
        bucket: <bucket_name>
        region: "<region_name>"
        # endpoint: <endpint_if_any>
        key_prefix: |-
          {{ print "audit-log/devtroncd/{{ container_name }}/%Y-%m-%d/" }}
        filename_append_uuid: false
        filename_time_format:  "%H_%M_%S"
        # auth:
        #   access_key_id: ""
        #   secret_access_key: ""
        compression: none
        batch:
          timeout_secs: 600
        encoding:
          codec: text
      my_sink_id_audit:
        type: aws_s3
        inputs:
          - my_remap_id
        bucket: <bucket_name>
        region: "<region_name>"
        # endpoint: <endpint_if_any>
        key_prefix: |-
          {{ print "vector-log-2/devtroncd/{{ container_name }}/%Y-%m-%d/" }}
        filename_append_uuid: false
        filename_time_format:  "%H_%M_%S"
        # auth:
        #   access_key_id: ""
        #   secret_access_key: ""
        compression: none
        batch:
          timeout_secs: 600 
        encoding:
          codec: text

uptime-kuma:
  enabled: false
  podEnv:
    - name: "UPTIME_KUMA_PORT"
      value: "3001"
  ingress:
    enabled: true
    className: "nginx"
    hosts:
    - host: "*"
  resources: {}
  volume:
    enabled: false
    accessMode: ReadWriteOnce
    size: 1Gi
    # storageClassName:
  serviceMonitor:
    enabled: true

jaeger-opentelemetry:
  enabled: false
  Deployment:
    Replicas: 1
    image: otel/opentelemetry-collector:0.83.0
  jaeger:
    replicas: 1
    image: jaegertracing/all-in-one:1.48
    storage: "10Gi"
    env: 
      - name: "BADGER_SPAN_STORE_TTL"
        value: "168h"
  service:
    type: ClusterIP
    port: 80 
  ingress:
    enabled: true
    className: "nginx"
    hosts:
      - host: "*.jaeger.devtron"

metrics-server:
  enabled: false
  replicas: 1
  resources: {}

prometheus-blackbox-exporter:
  enabled: false

k8s-event-logger:
  enabled: false

victoriametrics:
  enabled: false
  fullnameOverride: "victoria-metrics"
  victoria-metrics-operator:
    operator:
      disable_prometheus_converter: false
      prometheus_converter_add_argocd_ignore_annotations: true
  defaultDashboards:
    # -- Enable custom dashboards installation
    enabled: true
    defaultTimezone: ist
  vmsingle:
    enabled: true
    spec:
      retentionPeriod: "5d"
    ingress:
      enabled: true
      ingressClassName: nginx
      hosts:
      - vmsingle-stage.devtron
  alertmanager:
    enabled: true
    config:
      templates:
        - "/etc/vm/configs/**/*.tmpl"
      route:
      group_by: [ 'alertname', 'cluster', 'service' ]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'discord'
      routes:
      - receiver: 'null'
        matchers:
          - alertname =~ "InfoInhibitor|Watchdog|NginxHighHttp403ErrorRate.*"
      - receiver: 'null'
        matchers:
          - namespace !~ "devtroncd|devtron-ci|devtroncd-cd|opentelemetry|monitoring"
      - receiver: 'null'
        match:
          namespace: devtron-demo
      - receiver: 'null'
        match:
          namespace: demo-env
      - receiver: discord
        match:
          namespace: devtroncd
      - match:
          alertname: Watchdog
        receiver: dev-team-watchdog
      - match:
          team: devtron
        receiver: dev-team-discord
      - match:
          alertname: NodeCpuSaturation
        receiver: "null"
      - match:
          alertname: KubeCPUOvercommit
        receiver: "null"
      - match:
          alertname: CPUThrottlingHigh
        receiver: "null"
      - match:
          alertname: KubeContainerWaiting
        receiver: "null"
      - match:
          alertname: PrometheusRuleFailures
        receiver: "null"
      - match:
          alertname: KubeSchedulerDown
        receiver: "null"
      - match:
          alertname: KubeControllerManagerDown
        receiver: "null"
      - match:
          alertname: KubeClientCertificateExpiration
        receiver: "null"
      - match:
          alertname: NodeFilesystemSpaceFillingUp
        receiver: "null"
      - match:
          alertname: KubeProxyDown
        receiver: "null"
      - match:
          alertname: NoOutputBytesProcessed
        receiver: "null" 
      - match:
          alertname: TargetDown
          job: kubelet
          service: monitoring-prometheus-oper-kubelet
        receiver: "null"
      - match:
          job: kube-proxy
        receiver: "null"
      - match:
          namespace: "devtron-dev"
        receiver: "null"    
      - match:
          container: "metrics-server"
        receiver: "null"
      - match:
          alertname: ArgoCDAppDegraded
        receiver: null
      - match:
          alertname: ArgoCDAppMissing
        receiver: null   
      - match:
          alertname: ArgoCDAppUnknown
        receiver: null  
      - match:
          container: "fluent-bit"
        receiver: "null"  
      - match:
          alertname: KubeJobFailed
        receiver: "null"  
      - match:
        receiver: discord
        continue: true
      - match:
          severity: critical
        receiver: zenduty
      receivers:
        - name: "null"
        - name: dev-team-watchdog
          discord_configs:
          - webhook_url: ""
            send_resolved: true
            title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}-[<cluster_name>]'
        - name: dev-team-discord
          discord_configs:
          - webhook_url: ""
            send_resolved: true
            title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}-[<cluster_name>]'
        - name: discord
          discord_configs:
          - send_resolved: true
            webhook_url: ""
            title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}-[<cluster_name>]'
        - name: zenduty
          webhook_configs:
          - url: ""
  vmalert:
    enabled: true
    spec:
      extraArgs:
        external.url: https://vmalert.devtron
    ingress:
      enabled: true
      ingressClassName: nginx
      hosts:
        - vmalert.devtron
  vmagent:
    enabled: true
    spec:
      scrapeInterval: 20s
      externalLabels:
        cluster: cluster-name
        enterprise: testing
      extraArgs:
          promscrape.maxScrapeSize: "33554432"  # Increase max scrape size to 32MB
          promscrape.suppressScrapeErrors: "true"
          promscrape.suppressScrapeErrorsDelay: "30s"
    ingress:
      enabled: true
      ingressClassName: nginx-new
      hosts:
        - vmagent-stage.devtron
  grafana:
    enabled: true











